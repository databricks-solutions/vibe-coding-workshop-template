---
description: Guidelines for continuously improving Cursor rules based on emerging code patterns and best practices.
globs: **/*
alwaysApply: true
---
## Rule Improvement Triggers

- New code patterns not covered by existing rules
- Repeated similar implementations across files
- Common error patterns that could be prevented
- New libraries or tools being used consistently
- Emerging best practices in the codebase

# Analysis Process:
- Compare new code with existing rules
- Identify patterns that should be standardized
- Look for references to external documentation
- Check for consistent error handling patterns
- Monitor test patterns and coverage

# Rule Updates:

- **Add New Rules When:**
  - A new technology/pattern is used in 3+ files
  - Common bugs could be prevented by a rule
  - Code reviews repeatedly mention the same feedback
  - New security or performance patterns emerge

- **Modify Existing Rules When:**
  - Better examples exist in the codebase
  - Additional edge cases are discovered
  - Related rules have been updated
  - Implementation details have changed

- **Example Pattern Recognition:**

  **Real Example: Root Path for DLT Pipelines (October 2025)**
  
  ```yaml
  # Trigger: User provided official Databricks documentation about root_path
  # Analysis: Pattern missing from existing databricks-asset-bundles.mdc rule
  
  # BEFORE (incomplete pattern):
  resources:
    pipelines:
      silver_dlt_pipeline:
        name: "[${bundle.target}] Silver Layer Pipeline"
        catalog: ${var.catalog}
        schema: ${var.silver_schema}
        # Missing root_path configuration
  
  # AFTER (complete pattern added to rule):
  resources:
    pipelines:
      silver_dlt_pipeline:
        name: "[${bundle.target}] Silver Layer Pipeline"
        
        # âœ… ADDED: Pipeline root folder (Lakeflow Pipelines Editor best practice)
        root_path: ../src/altria_silver
        
        catalog: ${var.catalog}
        schema: ${var.silver_schema}
  
  # Actions Taken:
  # 1. Updated databricks-asset-bundles.mdc with root_path pattern
  # 2. Added new section: "Root Path Configuration (Lakeflow Pipelines Editor)"
  # 3. Updated validation checklist with 2 new root_path items
  # 4. Created 3 documentation guides (implementation, quick ref, summary)
  # 5. Applied pattern to existing codebase (silver_dlt_pipeline.yml)
  # 6. Documented the improvement process for future reference
  
  # See: docs/RULE_IMPROVEMENT_ROOT_PATH.md for complete case study
  ```

- **Rule Quality Checks:**
- Rules should be actionable and specific
- Examples should come from actual code
- References should be up to date
- Patterns should be consistently enforced

## Continuous Improvement:

- Monitor code review comments
- Track common development questions
- Update rules after major refactors
- Add links to relevant documentation
- Cross-reference related rules

## Rule Deprecation

- Mark outdated patterns as deprecated
- Remove rules that no longer apply
- Update references to deprecated rules
- Document migration paths for old patterns

## Documentation Updates:

- Keep examples synchronized with code
- Update references to external docs
- Maintain links between related rules
- Document breaking changes

## Rule Improvement Workflow

When you identify a pattern that should be added to cursor rules:

### 1. Validate the Trigger
- [ ] Is this pattern used in 3+ places, or from official documentation?
- [ ] Would this prevent common errors or improve quality?
- [ ] Is there clear benefit to standardizing this pattern?

### 2. Research the Pattern
- [ ] Find official documentation references
- [ ] Identify examples in current codebase
- [ ] Document benefits and trade-offs
- [ ] Check for related patterns already in rules

### 3. Update the Rule
- [ ] Add pattern to appropriate cursor rule file
- [ ] Include before/after examples
- [ ] Add to validation checklist if applicable
- [ ] Link to official documentation
- [ ] Use actual code examples from the project

### 4. Apply to Codebase
- [ ] Update existing code to follow new pattern
- [ ] Verify no linter errors introduced
- [ ] Test that pattern works as expected

### 5. Document the Improvement
- [ ] Create implementation guide (if complex)
- [ ] Create quick reference (for developer use)
- [ ] Document the improvement process itself
- [ ] Update related documentation with references

### 6. Knowledge Transfer
- [ ] Ensure pattern is discoverable
- [ ] Link from main documentation
- [ ] Add to validation checklists
- [ ] Consider team notification if breaking change

## Improvement Documentation Template

For significant rule improvements, create a case study document:

```markdown
# Rule Improvement Case Study: [Pattern Name]

**Date:** [Date]
**Rule Updated:** [rule-file.mdc]
**Trigger:** [What prompted this improvement]

## Trigger
- User request / External doc reference
- Pattern gap identified
- Official best practice available

## Analysis
- Official documentation reviewed
- Applicability to codebase assessed
- Benefits and risks identified

## Implementation
- [x] Configuration files updated
- [x] Cursor rules updated with pattern
- [x] Validation checklist items added
- [x] Supporting documentation created

## Results
- [Metrics: before/after comparison]
- [Knowledge transfer materials created]
- [Prevention: future issues avoided]

## Reusable Insights
- [What worked well]
- [Pattern recognition factors]
- [Replication strategy for similar improvements]
```

## Recent Rule Improvements

Track major improvements for reference:

- **Root Path for DLT Pipelines** (Oct 2025) - Added Lakeflow Pipelines Editor best practices to databricks-asset-bundles.mdc. See [docs/RULE_IMPROVEMENT_ROOT_PATH.md](../docs/RULE_IMPROVEMENT_ROOT_PATH.md)

- **Custom Metrics as Table Columns** (Oct 2025) - Critical learning about where custom metrics appear in Lakehouse Monitoring tables. Updated lakehouse-monitoring-business-drift-metrics.mdc with proper documentation patterns. See [docs/RULE_IMPROVEMENT_MONITORING_METRICS_COLUMNS.md](../docs/RULE_IMPROVEMENT_MONITORING_METRICS_COLUMNS.md)

- **Lakehouse Monitoring Setup Patterns** (Oct 2025) - Comprehensive patterns for production Lakehouse Monitoring including nested aggregation limitations, async wait patterns, complete cleanup, metric documentation sync, and silent success patterns. Updated lakehouse-monitoring-patterns.mdc with 5 major patterns. See [altriaDeltaShare/docs/RULE_IMPROVEMENT_LAKEHOUSE_MONITORING_SETUP.md](../altriaDeltaShare/docs/RULE_IMPROVEMENT_LAKEHOUSE_MONITORING_SETUP.md)

- **Table-Valued Functions SQL Patterns** (Oct 2025) - Created comprehensive patterns for Databricks TVFs optimized for Genie Spaces. Documented 3 critical SQL issues (parameter types, ordering, LIMIT clauses) discovered during production deployment of 15 functions. New rule: databricks-table-valued-functions.mdc. See [docs/RULE_IMPROVEMENT_TVF_SQL_PATTERNS.md](../docs/RULE_IMPROVEMENT_TVF_SQL_PATTERNS.md)

- **Lakehouse Monitoring Custom Metrics Query Patterns** (Oct 2025) - Discovered counter-intuitive storage behavior where custom metrics are stored by `input_columns` value, not always in `:table` row. Created comprehensive query patterns for AGGREGATE (PIVOT), DERIVED (Direct SELECT), and DRIFT metrics. Critical for AI/BI dashboard development. New rule: lakehouse-monitoring-custom-metrics-queries.mdc. See [docs/RULE_IMPROVEMENT_CUSTOM_METRICS_STORAGE.md](../docs/RULE_IMPROVEMENT_CUSTOM_METRICS_STORAGE.md)

- **Table-Level Business KPIs input_columns Pattern** (Oct 2025) - Critical discovery that ALL related metrics (AGGREGATE, DERIVED, DRIFT) must use `input_columns=[":table"]` for cross-referencing to work. DERIVED metrics can ONLY reference metrics in the same `column_name` row. Mixing per-column and table-level storage causes NULL values. Updated 83 AGGREGATE metrics and all three monitoring rules with decision tree and best practices. Query complexity reduced by 40%. See [altriaDeltaShare/docs/RULE_IMPROVEMENT_INPUT_COLUMNS_PATTERN.md](../altriaDeltaShare/docs/RULE_IMPROVEMENT_INPUT_COLUMNS_PATTERN.md)

- **Python File Imports After restartPython()** (Oct 2025) - Critical lesson: Pure Python files (.py) can be imported with standard imports, but Databricks notebooks (with `# Databricks notebook source` header) cannot. Initial overengineering attempted 6 complex workarounds before user correctly identified simple solution from official documentation. Remove notebook header to enable imports. New rule: databricks-python-imports.mdc (410+ lines). Updated lakehouse-monitoring-patterns.mdc with correct pattern. Key learning: Always check official documentation first before creating workarounds. See [altriaDeltaShare/docs/RULE_IMPROVEMENT_PYTHON_FILE_IMPORTS.md](../altriaDeltaShare/docs/RULE_IMPROVEMENT_PYTHON_FILE_IMPORTS.md)

- **DQX Framework Integration** (Oct 2025) - Comprehensive documentation of Databricks Labs DQX data quality framework for advanced validation with detailed diagnostics. Created complete integration patterns including hybrid DLT+DQX approach, YAML/Delta storage, Gold layer pre-merge validation, and metadata enrichment. New rule: dqx-patterns.mdc (410+ lines). Documented 3-phase rollout plan with pilot, storage, and validation phases. Key insight: Complement (don't replace) existing patterns - hybrid approach preserves DLT speed while adding DQX diagnostics. Total documentation: 1,710+ lines across 4 documents. See [altriaDeltaShare/docs/RULE_IMPROVEMENT_DQX_FRAMEWORK.md](../altriaDeltaShare/docs/RULE_IMPROVEMENT_DQX_FRAMEWORK.md)

- **DQX Gold Layer Production Implementation** (Oct 2025) - Production-hardened DQX patterns discovered through Gold layer pre-merge validation implementation. Documented 7 critical API compatibility issues and solutions: correct function names (`is_not_less_than` not `has_min`), parameter names (`limit` not `value`), data types (int not float), API method selection (`apply_checks_by_metadata_and_split`), Spark Connect compatibility (SQL queries not `sparkContext`), serverless library configuration (environment-level not task-level), and Delta table cleanup (DELETE-then-INSERT pattern). Updated dqx-patterns.mdc with comprehensive API reference, troubleshooting guide, and production-ready Gold layer pre-merge validation pattern. Successfully implemented 15 quality checks across 2 fact tables with quarantine strategy. Key learning: Always consult [official DQX API documentation](https://databrickslabs.github.io/dqx/docs/reference/api/check_funcs/) first - function names and parameters are strictly validated. Total updates: 500+ lines added to dqx-patterns.mdc. See [altriaDeltaShare/docs/RULE_IMPROVEMENT_DQX_GOLD_IMPLEMENTATION.md](../altriaDeltaShare/docs/RULE_IMPROVEMENT_DQX_GOLD_IMPLEMENTATION.md)

- **Ad-Hoc Exploration Notebooks** (Nov 2025) - Comprehensive pattern for creating dual-format exploration notebooks that work in both Databricks workspace and locally via Databricks Connect. Discovered through trial-and-error that magic commands (%pip, %sql, dbutils.library.restartPython()) only work in workspace, not Databricks Connect. Created dual format: .py for workspace with magic commands and widget fallback pattern, .ipynb for local Jupyter with direct variable assignment. Documented 5 critical issues: magic command syntax errors, missing spark session initialization, required .serverless()/.clusterId() configuration, widgets not available locally, and notebook path extensions in Asset Bundles. New rule: adhoc-exploration-notebooks.mdc (800+ lines). Includes 5 standard helper functions (list_tables, explore_table, check_data_quality, compare_tables, show_table_properties), requirements.txt, dual documentation (README + QUICKSTART), and Asset Bundle integration. Key learning: Execution environment fundamentally differs between workspace and Databricks Connect - need separate formats, not workarounds. Pattern saves ~2 hours per data product vs manual implementation. See [altriaDeltaShare/docs/RULE_IMPROVEMENT_EXPLORATION_NOTEBOOKS.md](../altriaDeltaShare/docs/RULE_IMPROVEMENT_EXPLORATION_NOTEBOOKS.md)

Follow [cursor-rules.mdc](mdc:.cursor/rules/cursor-rules.mdc) for proper rule formatting and structure.